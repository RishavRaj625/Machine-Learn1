
{
[
"import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# List of questions\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain the OOP concept in Python.\",\n",
    "    \"What is a list in Python?\",\n",
    "    \"How does a dictionary work in Python?\",\n",
    "    \"What are functions in Python?\",\n",
    "    \"What is inheritance in Python?\",\n",
    "    \"Explain decorators in Python.\",\n",
    "    \"How does exception handling work in Python?\",\n",
    "    \"What are generators in Python?\",\n",
    "    \"What is multithreading in Python?\"\n",
    "]\n",
    "\n",
    "# Step 1: Tokenize the questions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Step 2: Prepare input sequences\n",
    "input_sequences = []\n",
    "for line in questions:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Step 3: Pad the sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Step 4: Split input (X) and output (y)\n",
    "X = input_sequences[:,:-1]\n",
    "y = input_sequences[:,-1]\n",
    "\n",
    "# Step 5: One-hot encode the output labels (y)\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 6: Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Step 7: Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 8: Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Model summary to check the architecture\n",
    "model.summary()\n",
    "\n",
    "# Step 9: Function to generate new questions\n",
    "def generate_question(seed_text, next_words=5):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "# Example of generating a question\n",
    "seed_text = \"What is\"\n",
    "generated_question = generate_question(seed_text, next_words=5)\n",
    "print(\"Generated Question:\", generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0decc57-185e-4b4f-ae87-ffa2df3bc8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_question(seed_text, next_words=5)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4384d5-b01b-4b35-b762-7bafe9d9d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain the'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Explain the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417ea912-2270-41a1-92d9-988adfaa78a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_question(seed_text, next_words=5)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37aefa46-3ff9-4aab-b3b8-20c81d970722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is inheritance in python python python\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a question (defined earlier)\n",
    "def generate_question(seed_text, next_words=5):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "# Example: Ask a question starting with \"What is\"\n",
    "seed_text = \"What is\"\n",
    "generated_question = generate_question(seed_text, next_words=5)\n",
    "print(\"Generated Question:\", generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dab29c1-84f7-4f1d-9f47-4e5892929e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: Explain the oop concept in python python\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Explain the\"\n",
    "generated_question = generate_question(seed_text, next_words=5)\n",
    "print(\"Generated Question:\", generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "285807f5-2eb0-4899-aefa-e72c32d040a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: How does exception handling work in python\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"How does\"\n",
    "generated_question = generate_question(seed_text, next_words=5)\n",
    "print(\"Generated Question:\", generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12ce72cc-0ee3-4a30-8187-a0c2f5430a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: Explain the oop concept in python python python python python python\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Explain\"\n",
    "generated_question = generate_question(seed_text, next_words=10)\n",
    "print(\"Generated Question:\", generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "571631b9-5995-4303-8da0-823c1eeb9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Sampling\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_question(seed_text, next_words=5, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature sampling\n",
    "        predicted_probs = np.log(predicted_probs + 1e-7) / temperature\n",
    "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n",
    "        \n",
    "        predicted_word_index = np.random.choice(range(len(predicted_probs)), p=predicted_probs)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        \n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5effbad-380c-4e35-84dc-936d4e70c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is inheritance in python python python\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"What is\"\n",
    "generated_question = generate_question(seed_text, next_words=5, temperature=0.8)\n",
    "print(\"Generated Question:\", generated_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93a5d67c-1f89-4d42-b079-29d552fdc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(seed_text, next_words=5, temperature=1.0):\n",
    "    generated_words = set(seed_text.split())  # Track generated words\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature sampling\n",
    "        predicted_probs = np.log(predicted_probs + 1e-7) / temperature\n",
    "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n",
    "        \n",
    "        # Penalize previously generated words\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if word in generated_words:\n",
    "                predicted_probs[index] *= 0.01  # Reduce probability for repeated words\n",
    "\n",
    "        predicted_word_index = np.random.choice(range(len(predicted_probs)), p=predicted_probs)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        \n",
    "        seed_text += \" \" + predicted_word\n",
    "        generated_words.add(predicted_word)  # Add to the set of generated words\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeedc7b5-8ddc-48b0-972a-c5125c7c152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Generated Question: What is inheritance in python in\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def remove_repeated_words(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([words[i] for i in range(len(words)) if i == 0 or words[i] != words[i-1]])\n",
    "\n",
    "# Example of usage:\n",
    "seed_text = \"What is\"\n",
    "generated_question = generate_question(seed_text, next_words=5)\n",
    "cleaned_question = remove_repeated_words(generated_question)\n",
    "print(\"Cleaned Generated Question:\", cleaned_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddcf877a-7bdc-4d12-99f2-3c733835c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(seed_text, next_words=5, temperature=1.0):\n",
    "    generated_words = set(seed_text.split())  # Track generated words\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature sampling\n",
    "        predicted_probs = np.log(predicted_probs + 1e-7) / temperature\n",
    "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n",
    "        \n",
    "        # Penalize previously generated words\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if word in generated_words:\n",
    "                predicted_probs[index] *= 0.01  # Reduce probability for repeated words\n",
    "\n",
    "        # Normalize the probabilities to ensure they sum to 1\n",
    "        predicted_probs = predicted_probs / np.sum(predicted_probs)\n",
    "\n",
    "        # Pick the next word based on updated probabilities\n",
    "        predicted_word_index = np.random.choice(range(len(predicted_probs)), p=predicted_probs)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        \n",
    "        seed_text += \" \" + predicted_word\n",
    "        generated_words.add(predicted_word)  # Add to the set of generated words\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d77399c8-b722-4c86-9c34-aa583e965d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Generated Question: What is inheritance in python concept\n"
     ]
    }
   ],
   "source": [
    "# Example of usage\n",
    "seed_text = \"What is\"\n",
    "generated_question = generate_question(seed_text, next_words=5)\n",
    "cleaned_question = remove_repeated_words(generated_question)\n",
    "print(\"Cleaned Generated Question:\", cleaned_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770f4d8-ec6c-45f4-8d11-9b5a0b870530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
